# Copy this file to nilai/.env and set the values accordingly
# This file is used to set the environment variables for the project
# You shouldn't commit this file to the repository
# (!) You should replace the values with your own and not share them with anyone

# Hugging Face API Token
HF_TOKEN="Hugging Face API Token"

ENVIRONMENT = "mainnet"
NILAI_GUNICORN_WORKERS = 10
AUTH_STRATEGY = "api_key"

# Token to be whitelisted for queries
DOCS_TOKEN="Nillion2025"

# The domain name of the server
# - It must be written as "localhost" or "test.nilai.nillion"
# - Do not put "https://" or "http://" in the domain name or / at the end
NILAI_SERVER_DOMAIN = "localhost"

# Attestation Config
ATTESTATION_HOST = "attestation"
ATTESTATION_PORT = 8080

# nilAuth Trusted URLs
NILAUTH_TRUSTED_ROOT_ISSUERS = "http://localhost:30921"

# Postgres Docker Compose Config
POSTGRES_HOST = "postgres"
POSTGRES_USER = "user"
POSTGRES_PASSWORD = "password"
POSTGRES_DB = "mydb"
POSTGRES_DB_NUC = "mydb_nuc"
POSTGRES_DB_TESTNET = "mydb_testnet"
POSTGRES_PORT = 5432

# Redis Docker Compose Config
REDIS_URL = "redis://redis:6379"

# Etcd Docker Compose Config
ETCD_HOST = "etcd"
ETCD_PORT = 2379

# Grafana Docker Compose Config
GF_SECURITY_ADMIN_USER = "admin"
GF_SECURITY_ADMIN_PASSWORD = "password"

# WebSearch Settings
BRAVE_SEARCH_API = "Your API here"

# ============================================================================
# Security Configuration (v0.2.0+)
# ============================================================================

# CORS Origins (comma-separated, no spaces)
# CRITICAL: Never use "*" in production - specify exact origins
# Development default: http://localhost:3000,http://localhost:8080
# Production example: https://app.example.com,https://dashboard.example.com
CORS_ORIGINS = "http://localhost:3000,http://localhost:8080"

# Request Size Limit (bytes)
# Default: 10485760 (10MB)
# Increase if you need to support larger payloads
MAX_REQUEST_SIZE = 10485760

# Request Timeout (seconds)
# Default: 60 seconds
# Increase for long-running operations (not recommended)
REQUEST_TIMEOUT = 60

# ============================================================================
# vLLM Configuration (Optional - for model serving optimization)
# ============================================================================

# Tensor Parallelism Size
# Number of GPUs to split model across (default: 1)
# VLLM_TENSOR_PARALLEL_SIZE = 1

# Max Batched Tokens
# Maximum tokens in a batch (default: 8192)
# VLLM_MAX_NUM_BATCHED_TOKENS = 8192

# Max Number of Sequences
# Maximum concurrent sequences (default: 256)
# VLLM_MAX_NUM_SEQS = 256

# GPU Memory Utilization
# Fraction of GPU memory to use (default: 0.9)
# VLLM_GPU_MEMORY_UTILIZATION = 0.9

# Swap Space (GB)
# CPU-GPU swap space for overflow (default: 4)
# VLLM_SWAP_SPACE = 4

# Enforce Eager Mode
# Disable CUDA graphs for debugging (default: false)
# VLLM_ENFORCE_EAGER = false

# Disable Log Stats
# Disable periodic logging of statistics (default: false)
# VLLM_DISABLE_LOG_STATS = false
