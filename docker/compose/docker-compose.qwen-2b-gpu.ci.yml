version: "3.8"

services:
  c:
    image: nillion/nilai-vllm:latest
    container_name: qwen2vl_2b_gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ulimits:
      memlock: -1
      stack: 67108864
    env_file:
      - .env
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
    command:
      [
        "--model", "Qwen/Qwen2-VL-2B-Instruct-AWQ",
        "--model-impl", "vllm",
        "--tensor-parallel-size", "1",
        "--trust-remote-code",
        "--quantization", "awq",

        "--max-model-len", "1280",
        "--max-num-batched-tokens", "1280",
        "--max-num-seqs", "1",

        "--gpu-memory-utilization", "0.75",
        "--swap-space", "8",
        "--uvicorn-log-level", "warning",

        "--limit-mm-per-prompt", "{\"image\":1,\"video\":0}",
        "--skip-mm-profiling",
        "--enforce-eager"
      ]

    environment:
      SVC_HOST: qwen2vl_2b_gpu
      SVC_PORT: "8000"
      DISCOVERY_HOST: redis
      DISCOVERY_PORT: "6379"
      TOOL_SUPPORT: "true"
      MULTIMODAL_SUPPORT: "true"
      CUDA_LAUNCH_BLOCKING: "1"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
      PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True"
    volumes:
      - hugging_face_models:/root/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      retries: 3
      start_period: 60s
      timeout: 10s

volumes:
  hugging_face_models:
