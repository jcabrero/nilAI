# Fast API and serving
import json
import logging
import time
import uuid
from base64 import b64encode
from typing import AsyncGenerator, Optional, Union, List, Tuple
from nilai_api.attestation import get_attestation_report
from nilai_api.credit import LLMMeter, LLMUsage
from nilai_api.handlers.nilrag import handle_nilrag
from nilai_api.handlers.web_search import handle_web_search
from nilai_api.handlers.tools.tool_router import handle_tool_workflow

from fastapi import APIRouter, Body, Depends, HTTPException, status, Request
from fastapi.responses import StreamingResponse
from nilai_api.auth import get_auth_info, AuthenticationInfo
from nilai_api.config import CONFIG
from nilai_api.crypto import sign_message
from nilai_api.db.logs import QueryLogManager
from nilai_api.db.users import UserManager
from nilai_api.rate_limiting import RateLimit
from nilai_api.state import state

from nilai_api.handlers.nildb.api_model import (
    PromptDelegationRequest,
    PromptDelegationToken,
)
from nilai_api.handlers.nildb.handler import (
    get_nildb_delegation_token,
    get_prompt_from_nildb,
)

# Internal libraries
from nilai_common import (
    AttestationReport,
    ChatRequest,
    ModelMetadata,
    MessageAdapter,
    SignedChatCompletion,
    Source,
    Usage,
)

from nilauth_credit_middleware import MeteringContext
from openai import AsyncOpenAI


logger = logging.getLogger(__name__)

router = APIRouter()


@router.get("/v1/delegation")
async def get_prompt_store_delegation(
    prompt_delegation_request: PromptDelegationRequest,
    auth_info: AuthenticationInfo = Depends(get_auth_info),
) -> PromptDelegationToken:
    if not auth_info.user.is_subscription_owner:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail=f"Prompt storage is reserved to subscription owners: {auth_info.user} is not a subscription owner, apikey: {auth_info.user}",
        )

    try:
        return await get_nildb_delegation_token(prompt_delegation_request)
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Server unable to produce delegation tokens: {str(e)}",
        )


@router.get("/v1/usage", tags=["Usage"])
async def get_usage(auth_info: AuthenticationInfo = Depends(get_auth_info)) -> Usage:
    """
    Retrieve the current token usage for the authenticated user.

    - **user**: Authenticated user information (through HTTP Bearer header)
    - **Returns**: Usage statistics for the user's token consumption

    ### Example
    ```python
    # Retrieves token usage for the logged-in user
    usage = await get_usage(user)
    ```
    """
    return Usage(
        prompt_tokens=auth_info.user.prompt_tokens,
        completion_tokens=auth_info.user.completion_tokens,
        total_tokens=auth_info.user.prompt_tokens + auth_info.user.completion_tokens,
        queries=auth_info.user.queries,  # type: ignore # FIXME this field is not part of Usage
    )


@router.get("/v1/attestation/report", tags=["Attestation"])
async def get_attestation(
    auth_info: AuthenticationInfo = Depends(get_auth_info),
) -> AttestationReport:
    """
    Generate a cryptographic attestation report.

    - **attestation_request**: Attestation request containing a nonce
    - **user**: Authenticated user information (through HTTP Bearer header)
    - **Returns**: Attestation details for service verification

    ### Attestation Details
    - `verifying_key`: Public key for cryptographic verification
    - `cpu_attestation`: CPU environment verification
    - `gpu_attestation`: GPU environment verification

    ### Security Note
    Provides cryptographic proof of the service's integrity and environment.
    """

    attestation_report = await get_attestation_report()
    attestation_report.verifying_key = state.b64_public_key
    return attestation_report


@router.get("/v1/models", tags=["Model"])
async def get_models(
    auth_info: AuthenticationInfo = Depends(get_auth_info),
) -> List[ModelMetadata]:
    """
    List all available models in the system.

    - **user**: Authenticated user information (through HTTP Bearer header)
    - **Returns**: Dictionary of available models

    ### Example
    ```python
    # Retrieves list of available models
    models = await get_models(user)
    ```
    """
    return [endpoint.metadata for endpoint in (await state.models).values()]


async def chat_completion_concurrent_rate_limit(request: Request) -> Tuple[int, str]:
    body = await request.json()
    try:
        chat_request = ChatRequest(**body)
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid request body")
    key = f"chat:{chat_request.model}"
    limit = CONFIG.rate_limiting.model_concurrent_rate_limit.get(
        chat_request.model,
        CONFIG.rate_limiting.model_concurrent_rate_limit.get("default", 50),
    )
    return limit, key


async def chat_completion_web_search_rate_limit(request: Request) -> bool:
    """Extract web_search flag from request body for rate limiting."""
    body = await request.json()
    try:
        chat_request = ChatRequest(**body)
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid request body")
    return bool(chat_request.web_search)


@router.post("/v1/chat/completions", tags=["Chat"], response_model=None)
async def chat_completion(
    req: ChatRequest = Body(
        ChatRequest(
            model="meta-llama/Llama-3.2-1B-Instruct",
            messages=[
                MessageAdapter.new_message(
                    role="system", content="You are a helpful assistant."
                ),
                MessageAdapter.new_message(role="user", content="What is your name?"),
            ],
        )
    ),
    _rate_limit=Depends(
        RateLimit(
            concurrent_extractor=chat_completion_concurrent_rate_limit,
            web_search_extractor=chat_completion_web_search_rate_limit,
        )
    ),
    auth_info: AuthenticationInfo = Depends(get_auth_info),
    meter: MeteringContext = Depends(LLMMeter),
) -> Union[SignedChatCompletion, StreamingResponse]:
    """
    Generate a chat completion response from the AI model.

    - **req**: Chat completion request containing messages and model specifications
    - **user**: Authenticated user information (through HTTP Bearer header)
    - **Returns**: Full chat response with model output, usage statistics, and cryptographic signature

    ### Request Requirements
    - Must include non-empty list of messages
    - Must specify a model
    - Supports multiple message formats (system, user, assistant)
    - Optional web_search parameter to enhance context with current information

    ### Response Components
    - Model-generated text completion
    - Token usage metrics
    - Cryptographically signed response for verification

    ### Processing Steps
    1. Validate input request parameters
    2. If web_search is enabled, perform web search and enhance context
    3. Prepare messages for model processing
    4. Generate AI model response
    5. Track and update token usage
    6. Cryptographically sign the response

    ### Web Search Feature
    When web_search=True, the system will:
    - Extract the user's query from the last user message
    - Perform a web search using Brave API
    - Enhance the conversation context with current information
    - Add search results as a system message for better responses

    ### Potential HTTP Errors
    - **400 Bad Request**:
      - Missing messages list
      - No model specified
    - **500 Internal Server Error**:
      - Model fails to generate a response

    ### Example
    ```python
    # Generate a chat completion with web search
    request = ChatRequest(
        model="meta-llama/Llama-3.2-1B-Instruct",
        messages=[
            {"role": "system", "content": "You are a helpful assistant"},
            {"role": "user", "content": "What is your name?"}
        ],
    )
    response = await chat_completion(request, user)
    """

    if len(req.messages) == 0:
        raise HTTPException(
            status_code=400,
            detail="Request contained 0 messages",
        )
    model_name = req.model
    request_id = str(uuid.uuid4())
    t_start = time.monotonic()
    logger.info(f"[chat] call start request_id={req.messages}")
    endpoint = await state.get_model(model_name)
    if endpoint is None:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Invalid model name {model_name}, check /v1/models for options",
        )

    if not endpoint.metadata.tool_support and req.tools:
        raise HTTPException(
            status_code=400,
            detail="Model does not support tool usage, remove tools from request",
        )

    has_multimodal = req.has_multimodal_content()
    logger.info(f"[chat] has_multimodal: {has_multimodal}")
    if has_multimodal and (not endpoint.metadata.multimodal_support or req.web_search):
        raise HTTPException(
            status_code=400,
            detail="Model does not support multimodal content, remove image inputs from request",
        )

    model_url = endpoint.url + "/v1/"

    logger.info(
        f"[chat] start request_id={request_id} user={auth_info.user.userid} model={model_name} stream={req.stream} web_search={bool(req.web_search)} tools={bool(req.tools)} multimodal={has_multimodal} url={model_url}"
    )

    client = AsyncOpenAI(base_url=model_url, api_key="<not-needed>")
    if auth_info.prompt_document:
        try:
            nildb_prompt: str = await get_prompt_from_nildb(auth_info.prompt_document)
            req.messages.insert(
                0, MessageAdapter.new_message(role="system", content=nildb_prompt)
            )
        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Unable to extract prompt from nilDB: {str(e)}",
            )

    if req.nilrag:
        logger.info(f"[chat] nilrag start request_id={request_id}")
        t_nilrag = time.monotonic()
        await handle_nilrag(req)
        logger.info(
            f"[chat] nilrag done request_id={request_id} duration_ms={(time.monotonic() - t_nilrag) * 1000:.0f}"
        )

    messages = req.messages
    sources: Optional[List[Source]] = None

    if req.web_search:
        logger.info(f"[chat] web_search start request_id={request_id}")
        t_ws = time.monotonic()
        web_search_result = await handle_web_search(req, model_name, client)
        messages = web_search_result.messages
        sources = web_search_result.sources
        logger.info(
            f"[chat] web_search done request_id={request_id} sources={len(sources) if sources else 0} duration_ms={(time.monotonic() - t_ws) * 1000:.0f}"
        )
        logger.info(f"[chat] web_search messages: {messages}")

    if req.stream:

        async def chat_completion_stream_generator() -> AsyncGenerator[str, None]:
            t_call = time.monotonic()
            prompt_token_usage = 0
            completion_token_usage = 0

            try:
                logger.info(f"[chat] stream start request_id={request_id}")

                request_kwargs = {
                    "model": req.model,
                    "messages": messages,
                    "stream": True,
                    "top_p": req.top_p,
                    "temperature": req.temperature,
                    "max_tokens": req.max_tokens,
                    "extra_body": {
                        "stream_options": {
                            "include_usage": True,
                            "continuous_usage_stats": False,
                        }
                    },
                }
                if req.tools:
                    request_kwargs["tools"] = req.tools

                response = await client.chat.completions.create(**request_kwargs)

                async for chunk in response:
                    if chunk.usage is not None:
                        prompt_token_usage = chunk.usage.prompt_tokens
                        completion_token_usage = chunk.usage.completion_tokens

                    payload = chunk.model_dump(exclude_unset=True)

                    if chunk.usage is not None and sources:
                        payload["sources"] = [
                            s.model_dump(mode="json") for s in sources
                        ]

                    yield f"data: {json.dumps(payload)}\n\n"

                await UserManager.update_token_usage(
                    auth_info.user.userid,
                    prompt_tokens=prompt_token_usage,
                    completion_tokens=completion_token_usage,
                )
                meter.set_response(
                    {
                        "usage": LLMUsage(
                            prompt_tokens=prompt_token_usage,
                            completion_tokens=completion_token_usage,
                            web_searches=len(sources) if sources else 0,
                        )
                    }
                )
                await QueryLogManager.log_query(
                    auth_info.user.userid,
                    model=req.model,
                    prompt_tokens=prompt_token_usage,
                    completion_tokens=completion_token_usage,
                    web_search_calls=len(sources) if sources else 0,
                )
                logger.info(
                    "[chat] stream done request_id=%s prompt_tokens=%d completion_tokens=%d "
                    "duration_ms=%.0f total_ms=%.0f",
                    request_id,
                    prompt_token_usage,
                    completion_token_usage,
                    (time.monotonic() - t_call) * 1000,
                    (time.monotonic() - t_start) * 1000,
                )

            except Exception as e:
                logger.error(
                    "[chat] stream error request_id=%s error=%s", request_id, e
                )
                yield f"data: {json.dumps({'error': 'stream_failed', 'message': str(e)})}\n\n"

        return StreamingResponse(
            chat_completion_stream_generator(),
            media_type="text/event-stream",
        )

    current_messages = messages
    request_kwargs = {
        "model": req.model,
        "messages": current_messages,  # type: ignore
        "top_p": req.top_p,
        "temperature": req.temperature,
        "max_tokens": req.max_tokens,
    }
    if req.tools:
        request_kwargs["tools"] = req.tools  # type: ignore
        request_kwargs["tool_choice"] = req.tool_choice

    logger.info(f"[chat] call start request_id={request_id}")
    logger.info(f"[chat] call message: {current_messages}")
    t_call = time.monotonic()
    response = await client.chat.completions.create(**request_kwargs)  # type: ignore
    logger.info(
        f"[chat] call done request_id={request_id} duration_ms={(time.monotonic() - t_call) * 1000:.0f}"
    )
    logger.info(f"[chat] call response: {response}")

    # Handle tool workflow fully inside tools.router
    (
        final_completion,
        agg_prompt_tokens,
        agg_completion_tokens,
    ) = await handle_tool_workflow(client, req, current_messages, response)
    logger.info(f"[chat] call final_completion: {final_completion}")
    model_response = SignedChatCompletion(
        **final_completion.model_dump(),
        signature="",
        sources=sources,
    )

    logger.info(
        f"[chat] model_response request_id={request_id} duration_ms={(time.monotonic() - t_call) * 1000:.0f}"
    )

    if model_response.usage is None:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Model response does not contain usage statistics",
        )

    if agg_prompt_tokens or agg_completion_tokens:
        total_prompt_tokens = response.usage.prompt_tokens
        total_completion_tokens = response.usage.completion_tokens

        total_prompt_tokens += agg_prompt_tokens
        total_completion_tokens += agg_completion_tokens

        model_response.usage.prompt_tokens = total_prompt_tokens
        model_response.usage.completion_tokens = total_completion_tokens
        model_response.usage.total_tokens = (
            total_prompt_tokens + total_completion_tokens
        )

    # Update token usage in DB
    await UserManager.update_token_usage(
        auth_info.user.userid,
        prompt_tokens=model_response.usage.prompt_tokens,
        completion_tokens=model_response.usage.completion_tokens,
    )
    meter.set_response(
        {
            "usage": LLMUsage(
                prompt_tokens=model_response.usage.prompt_tokens,
                completion_tokens=model_response.usage.completion_tokens,
                web_searches=len(sources) if sources else 0,
            )
        }
    )
    await QueryLogManager.log_query(
        auth_info.user.userid,
        model=req.model,
        prompt_tokens=model_response.usage.prompt_tokens,
        completion_tokens=model_response.usage.completion_tokens,
        web_search_calls=len(sources) if sources else 0,
    )

    # Sign the response
    response_json = model_response.model_dump_json()
    signature = sign_message(state.private_key, response_json)
    model_response.signature = b64encode(signature).decode()

    logger.info(
        f"[chat] done request_id={request_id} prompt_tokens={model_response.usage.prompt_tokens} completion_tokens={model_response.usage.completion_tokens} total_ms={(time.monotonic() - t_start) * 1000:.0f}"
    )
    return model_response
